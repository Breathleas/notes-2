\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{CJK}    % chinese character
\usepackage{dsfont}  % \mathds{}
\usepackage{amssymb}  %  \mathbb{}

\usepackage{fancyhdr}
\pagestyle{fancy} % enable page headers
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=3.5cm,bottom=2.5cm}
% ----------------------------------------------------------------
\begin{document}
\fancyhead{} % clear all fields
\lhead{5.1    Neural Networks: Cost function and Backpropagation}
\title{Neural Networks: Cost function and Backpropagation}%
\author{Di Zhao, 2017-4-23}%
\date{\small{\texttt{zhaodi01@mail.ustc.edu.cn}}}%
% ----------------------------------------------------------------

\maketitle
\thispagestyle{fancy}
% ----------------------------------------------------------------

\begin{CJK*}{GBK}{song}

\section{Model}

Some variables:

$L$ = 总层数。 total number of layers in the network

$s_l$ = 第l层单元数。 number of units (not counting bias unit) in layer l

$K$ = 输出类别数。number of output units/classes\\


训练集：

$\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\}$\\


\section{classification}

\subsection{binary classification}

$y\ =\ 0$ or 1:\\

$h_{\Theta}(x) \in \mathds{R}$ \kern2cm $K=1$

\subsection{Multi-class classification}

$y\ \in\ \mathds{R}^{K}$ :\\

$h_{\Theta}(x) \in \mathds{R}^{K}$ \kern2cm $s_l=K$

\subsection{Cost Function}

回顾正规化后的逻辑回归损失函数（cost function）：

$J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^{m}
[y^{(i)}log(h_{\theta}(x^{(i)}))
+ (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]
+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}(\theta_{j})^2$\\\\

将$h_{\Theta}(x)_k$记为第k个输出，则神经网络的损失函数为：

$J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}
[y_k^{(i)}log((h_{\Theta}(x^{(i)}))_k)
+ (1-y_k^{(i)})log(1-(h_{\Theta}(x^{(i)}))_k)]
+\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}
\sum\limits_{j=1}^{s_{l+1}}(\Theta_{j,i}^{(l)})^2$\\\\

改变之处：左侧增加了对输出节点的求和；右侧的正规化部分，将所有层的
$\theta$都计算在内。\\

\section{Backpropagation}

本节介绍最小化cost function的算法：Backpropagation。回顾cost function
公式：
\[
\begin{split}
J(\theta)= & -\frac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}
[y_k^{(i)}log((h_{\Theta}(x^{(i)}))_k)
+ (1-y_k^{(i)})log(1-(h_{\Theta}(x^{(i)}))_k)]\\
& +\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}
\sum\limits_{j=1}^{s_{l+1}}(\Theta_{j,i}^{(l)})^2
\end{split}
\]
为了获得使$J(\Theta)$最小的$\Theta$，需要计算$J(\Theta)$的偏导
$-\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$。

\subsection{Gradient computation}

对一个四层网络，对训练样本(x, y)，向前传播过程：\\

(第一层的activation value) $a^{(1)}=x$\\

\ \kern4cm$a^{(2)}=g(\Theta^{(1)}a^{(1)})$\  [增加$a_0^{(2)}$]\\

\ \kern4cm$a^{(3)}=g(\Theta^{(2)}a^{(2)})$\  [增加$a_0^{(3)}$]\\

\ \kern4cm$a^{(4)}=g(\Theta^{(3)}a^{(3)})=h_{\Theta}(x)$\\

根据直觉， \underline{向后传播要计算每个$\delta_j^{(l)}$：
第$l$层节点$j$的误差，即“激励值”$a_j^{(l)}$的误差。}\\

则对输出单元（第四层），$\delta_j^{(4)}=a_j^{(4)}-y_j$。向量形式：
$\ \delta^{(4)}=a^{(4)}-y$。则：\\

\ \kern2cm$\delta^{(3)}
=(\Theta^{(3)})^T\delta^{(4)}.\ast g^{'}(z^{(3)})$\\

\ \kern2cm$\delta^{(2)}
=(\Theta^{(2)})^T\delta^{(3)}.\ast g^{'}(z^{(2)})$\\

其中，$.\ast$表示点乘；$(\Theta^{(3)})^T\delta^{(4)}$与
$g^{'}(z^{(3)})$均为向量。\\

\begin{center}
\dashbox{5}(350,120)[s]{
  \parbox{12cm}{
尝试推导：
\[
\begin{split}
a^{(4)}=&\ g(\Theta^{(3)}a^{(3)})=g(z^{(3)}),\\
y=&\ g(\Theta^{(3)}(a^{(3)}-\delta^{(3)}))=
 g(z^{(3)}-\Theta^{(3)}\delta^{(3)}))\\
=> \kern1cm &\\
\delta^{(4)}=a^{(4)}-y=&
\ g(z^{(3)})-g(z^{(3)}-\Theta^{(3)}\delta^{(3)}))\\
\approx &\ g^{'}(z^{(3)})\ (\Theta^{(3)}\delta^{(3)})
\end{split}
\]
  }
}
\end{center}

经计算有：\\

\ \kern2cm$g^{'}(z^{(3)})=a^{(3)}.\ast (1-a^{(3)})$\\

\ \kern2cm$g^{'}(z^{(2)})=a^{(2)}.\ast (1-a^{(2)})$\\

向后传播的含义就是根据最后一层得到的误差一层层计算前面
的误差。（第一层为输入层，不计算误差。）

在不计正规化参数($\lambda=0$)时:

\begin{center}
$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=
a_j^{(l)}\delta_i^{(l+1)}$
\end{center}

\kern1cm

\subsection{Backpropagation algorithm}

Training set: $\{(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})\}$\\

对所有l,i,j，设$\Delta_{ij}^{(l)}=0$。
用以计算$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$。
($\Delta$为$\delta$的大写)

计算过程：\\


\fbox{%
\parbox{16cm}{%
For $i=1$ to $m$ :(对每个训练样本进行计算)
    \begin{quote}
    设$a^{(1)}=x^{(i)}$

    通过向前传播计算$a^{(l)}$，$l=2, 3, ..., L$.

    使用$y^{(i)}$，计算$\delta^{(L)}=a^{(L)}-y^{(i)}$.
    
    向后传播计算$\delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}$.
    (由于不计输入的误差， 因此没有$\delta^{(1)}$.)
    
    累加得到的偏导项：
    $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$
    
    (向量形式：
    $\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$.)
    \end{quote}
    
循环结束后，计算：\\

$D_{ij}^{(l)}:=
    \frac{1}{m}\Delta_{ij}{(l)}+\lambda \Theta_{ij}{(l)}$
    \kern1cm if $j\neq0$\\

$D_{ij}^{(l)}:=
    \frac{1}{m}\Delta_{ij}{(l)}$ \kern1cm if $j=0$

}
}

\kern5mm

可以证明， $\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)
=D_{ij}^{(l)}$. 因此，可使用这些值进行梯度下降。\\\\

\section{Backpropagation Intuition}

对于神经网络损失函数
\[
\begin{split}
J(\theta)= & -\frac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}
[y_k^{(i)}log((h_{\Theta}(x^{(i)}))_k)
+ (1-y_k^{(i)})log(1-(h_{\Theta}(x^{(i)}))_k)]\\
& +\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}
\sum\limits_{j=1}^{s_{l+1}}(\Theta_{j,i}^{(l)})^2
\end{split}
\]
忽略正规化项($\lambda = 0$)，并且只考虑单类分类问题，则：\\

$J(\theta)=  -\frac{1}{m}\sum\limits_{i=1}^{m}
[y^{(i)}log((h_{\Theta}(x^{(i)})))
+ (1-y^{(i)})log(1-(h_{\Theta}(x^{(i)})))]$\\\\

于是，每个样本的贡献为：\\

$cost(i)= y^{(i)}log((h_{\Theta}(x^{(i)})))
+ (1-y^{(i)})log(1-(h_{\Theta}(x^{(i)})))$\\\\

直觉上，$\delta_j^{(l)}$为$a_j^{(l)}$产生的误差。实际上，
$\delta$值为损失函数的偏导：\\

\ \kern1cm
$\delta_j^{(l)}=\frac{\partial}{\partial z_j^{(l)}}cost(i)$
\kern1cm [对样本i]\\

对于一个第三层有两个节点(不含bias unit)的神经网络，反向传播中：\\

\ \kern1cm
$\delta_2^{(2)}=\Theta_{12}^{(2)}\ast\delta_1^{(3)}
+ \Theta_{22}^{(2)}\ast\delta_2^{(3)}$

\end{CJK*}

\end{document}

